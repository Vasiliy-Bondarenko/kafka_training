# !!!!!!!!! NOTE !!!!!!!!!!!
# pass-through variables from environment into containers with ${VAR}
# start services with ./start.sh script
---
version: '3'
services:
  connect-ui:
    # docs: https://github.com/lensesio/kafka-connect-ui
    image: landoop/kafka-connect-ui
    ports:
    - 8007:8000
    environment:
      CONNECT_URL: http://connect:8083
    depends_on:
      - connect
  connect:
    image: confluentinc/cp-kafka-connect:5.4.1
    hostname: connect
    container_name: connect
    ports:
      - "8083:8083"
    volumes:
      - ./kafka_connect:/app
    command: /app/start.sh
    environment:
      # settings taken mostly from: https://docs.confluent.io/4.0.0/cloud/connect/connect-cloud-config.html#standalone-cluster
      CONNECT_CONNECTOR_CLIENT_CONFIG_OVERRIDE_POLICY: All

      CONNECT_BOOTSTRAP_SERVERS: ${BROKER_URL}

      CONNECT_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_SASL_MECHANISM: PLAIN
      CONNECT_REQUEST_TIMEOUT_MS: 10000
      CONNECT_RETRY_BACKOFF_MS: 500
      CONNECT_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${BROKER_KEY}" password="${BROKER_SECRET}" serviceName="kafka";
      CONNECT_SECURITY_PROTOCOL: SASL_SSL

      CONNECT_CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_CONSUMER_SASL_MECHANISM: PLAIN
      CONNECT_CONSUMER_REQUEST_TIMEOUT_MS: 10000
      CONNECT_CONSUMER_RETRY_BACKOFF_MS: 500
      CONNECT_CONSUMER_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${BROKER_KEY}" password="${BROKER_SECRET}" serviceName="kafka";
      CONNECT_CONSUMER_SECURITY_PROTOCOL: SASL_SSL

      CONNECT_PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONNECT_PRODUCER_SASL_MECHANISM: PLAIN
      CONNECT_PRODUCER_REQUEST_TIMEOUT_MS: 10000
      CONNECT_PRODUCER_RETRY_BACKOFF_MS: 500
      CONNECT_PRODUCER_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${BROKER_KEY}" password="${BROKER_SECRET}" serviceName="kafka";
      CONNECT_PRODUCER_SECURITY_PROTOCOL: SASL_SSL

      CONNECT_GROUP_ID: compose-connect-group-v.0.1
      CONNECT_CONFIG_STORAGE_REPLICATION_FACTOR: 3
      CONNECT_CONFIG_STORAGE_TOPIC: docker_kafka_connect_config
      CONNECT_OFFSET_STORAGE_TOPIC: docker_kafka_connect_offset
      CONNECT_STATUS_STORAGE_TOPIC: docker_kafka_connect_status
      CONNECT_OFFSET_FLUSH_INTERVAL_MS: 10000
      # CLASSPATH required due to CC-2422
      CLASSPATH: /usr/share/java/monitoring-interceptors/monitoring-interceptors-5.4.1.jar
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_INTERNAL_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_REST_ADVERTISED_HOST_NAME: connect

      SCHEMA_REGISTRY_ENDPOINT: ${SCHEMA_REGISTRY_ENDPOINT}
      SCHEMA_REGISTRY_KEY: ${SCHEMA_REGISTRY_KEY}
      SCHEMA_REGISTRY_SECRET: ${SCHEMA_REGISTRY_SECRET}

      KALEIDO_SUBMIT_TOPIC: ${KALEIDO_SUBMIT_TOPIC}
      KALEIDO_RECEIVE_TOPIC: ${KALEIDO_RECEIVE_TOPIC}
  kafka_faker:
    build: ./kafka_faker
    entrypoint: |
      bash -c "
      tail -f /dev/null"
    volumes:
      - ./kafka_faker:/app
    ports:
      - 6066:6066
    environment:
      WORKER_PORT: 6066
      KAFKA_BOOTSTRAP_SERVER: ${BROKER_URL}
      BROKER_KEY: ${BROKER_KEY}
      BROKER_SECRET: ${BROKER_SECRET}
      SSL_ENABLED: 1

      KALEIDO_SUBMIT_TOPIC: ${KALEIDO_SUBMIT_TOPIC}
      KALEIDO_RECEIVE_TOPIC: ${KALEIDO_RECEIVE_TOPIC}

      SCHEMA_REGISTRY_ENDPOINT: ${SCHEMA_REGISTRY_ENDPOINT}
      SCHEMA_REGISTRY_KEY: ${SCHEMA_REGISTRY_KEY}
      SCHEMA_REGISTRY_SECRET: ${SCHEMA_REGISTRY_SECRET}
  kafka_processors:
    build: ./kafka_processors
#    entrypoint: |
#      bash -c "
#      tail -f /dev/null"
    volumes:
      - ./kafka_processors:/app
    ports:
      - 6067:6066
    environment:
      WORKER_PORT: 6066
      KAFKA_BOOTSTRAP_SERVER: ${BROKER_URL}
      BROKER_KEY: ${BROKER_KEY}
      BROKER_SECRET: ${BROKER_SECRET}
      SSL_ENABLED: 1

      KALEIDO_SUBMIT_TOPIC: ${KALEIDO_SUBMIT_TOPIC}
      KALEIDO_RECEIVE_TOPIC: ${KALEIDO_RECEIVE_TOPIC}

      SCHEMA_REGISTRY_ENDPOINT: ${SCHEMA_REGISTRY_ENDPOINT}
      SCHEMA_REGISTRY_KEY: ${SCHEMA_REGISTRY_KEY}
      SCHEMA_REGISTRY_SECRET: ${SCHEMA_REGISTRY_SECRET}
  db:
    image: postgres
    ports:
      - "8032:5432"
    environment:
      POSTGRES_PASSWORD: open
  eth_gateway_submit:
    build: ./eth_gateway
#    entrypoint: |
#      bash -c "
#      tail -f /dev/null"
#    volumes:
#      - ./eth_gateway/configs:/etc/kafka-mirrormaker
    environment:
      # [2020-09-29 11:21:12,736] ERROR Error when sending message to topic e0zgfop4ib-e0b4hhlppa-requests with key: null, value: 531 bytes with error: (org.apache.kafka.clients.producer.internals.ErrorLoggingCallback)
      #org.apache.kafka.common.errors.TimeoutException: Expiring 30 record(s) for e0zgfop4ib-e0b4hhlppa-requests-0: 260985 ms has passed since last append
      # то ли он не пытается отправить батч, то ли не может отправить и отваливается с таймаутом

      MIRRORMAKER_WHITE_LIST: ${KALEIDO_SUBMIT_TOPIC}
      MIRRORMAKER_ABORT_ON_SEND_FAILURE: "true"
      MIRRORMAKER_OFFSET_COMMIT_INTERVAL: 10000
      MIRRORMAKER_NUM_STREAMS: 1
#      MIRRORMAKER_CONSUMER_REBALANCE_LISTENER: org.mypackage.MyListener
#      MIRRORMAKER_CONSUMER_REBALANCE_LISTENER_ARGS: arg1,arg2
#      MIRRORMAKER_MESSAGE_HANDLER: org.mypackage.MyHandler
#      MIRRORMAKER_MESSAGE_HANDLER_ARGS: arg1,arg2

      CONSUMER_BOOTSTRAP_SERVERS: ${BROKER_URL}

      # Consumer authentication
      CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONSUMER_SASL_MECHANISM: PLAIN
      CONSUMER_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${BROKER_KEY}" password="${BROKER_SECRET}" serviceName="kafka";
      CONSUMER_SECURITY_PROTOCOL: SASL_SSL

      CONSUMER_GROUP_ID: MirrorMaker.v.0.1
      CONSUMER_AUTO_OFFSET_RESET: earliest # resubmit all messages on init and in case offset is lost
      CONSUMER_REQUEST_TIMEOUT_MS: 20000
      CONSUMER_RETRY_BACKOFF_MS: 500

      PRODUCER_BOOTSTRAP_SERVERS: ${KALEIDO_BROKER_URLS}

      # Producer authentication
      PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      PRODUCER_SASL_MECHANISM: PLAIN
      PRODUCER_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${KALEIDO_BROKER_KEY}" password="${KALEIDO_BROKER_SECRET}" serviceName="kafka";
      PRODUCER_SECURITY_PROTOCOL: SASL_SSL

      # Producer config: https://docs.confluent.io/current/installation/configuration/producer-configs.html
      PRODUCER_REQUEST_TIMEOUT_MS: 30000 # 30 sec (will retry after that)
      PRODUCER_RETRY_BACKOFF_MS: 2000
      PRODUCER_MAX_BLOCK_MS: 60000
      PRODUCER_LINGER_MS: 1000 # Batch all messages for 1 second before sending out.
      PRODUCER_BUFFER_MEMORY: 33554432 # Max memory used for buffering before blocking for max.block.ms
      PRODUCER_BATCH_SIZE: 500000 # In bytes
      PRODUCER_DELIVERY_TIMEOUT_MS: 120000 # 3 minutes
      PRODUCER_MAX_REQUEST_SIZE: 1048576 # (can be a few batches in 1 request)
#      PRODUCER_ENABLE_IDEMPOTENCE: "true" # NOT WORKING WITH KALEIDO! Lower the chance of transaction duplicates (they are still possible on Kaleido side)

      PRODUCER_RETRIES: 5
      PRODUCER_MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION: 1 # set to 1 to avoid reordering messages
      PRODUCER_ACKS: "all"

#      KALEIDO_SUBMIT_TOPIC: ${KALEIDO_SUBMIT_TOPIC}
  eth_gateway_receive:
    build: ./eth_gateway
    #    entrypoint: |
    #      bash -c "
    #      tail -f /dev/null"
    #    volumes:
    #      - ./eth_gateway/configs:/etc/kafka-mirrormaker
    environment:
      MIRRORMAKER_WHITE_LIST: ${KALEIDO_RECEIVE_TOPIC}
      MIRRORMAKER_ABORT_ON_SEND_FAILURE: "true"
      MIRRORMAKER_OFFSET_COMMIT_INTERVAL: 60000
      MIRRORMAKER_NUM_STREAMS: 1
#      MIRRORMAKER_CONSUMER_REBALANCE_LISTENER: org.mypackage.MyListener
#      MIRRORMAKER_CONSUMER_REBALANCE_LISTENER_ARGS: arg1,arg2
#      MIRRORMAKER_MESSAGE_HANDLER: org.mypackage.MyHandler
#      MIRRORMAKER_MESSAGE_HANDLER_ARGS: arg1,arg2

      CONSUMER_BOOTSTRAP_SERVERS: ${KALEIDO_BROKER_URLS}
      CONSUMER_GROUP_ID: MirrorMaker.v.0.1
      CONSUMER_AUTO_OFFSET_RESET: earliest # resubmit all messages on init and in case offset is lost
      CONSUMER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      CONSUMER_SASL_MECHANISM: PLAIN
      CONSUMER_REQUEST_TIMEOUT_MS: 20000
      CONSUMER_RETRY_BACKOFF_MS: 500
      CONSUMER_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${KALEIDO_BROKER_KEY}" password="${KALEIDO_BROKER_SECRET}" serviceName="kafka";
      CONSUMER_SECURITY_PROTOCOL: SASL_SSL

      PRODUCER_BOOTSTRAP_SERVERS: ${BROKER_URL}
      PRODUCER_SSL_ENDPOINT_IDENTIFICATION_ALGORITHM: https
      PRODUCER_SASL_MECHANISM: PLAIN
      PRODUCER_SASL_JAAS_CONFIG: org.apache.kafka.common.security.plain.PlainLoginModule required username="${BROKER_KEY}" password="${BROKER_SECRET}" serviceName="kafka";
      PRODUCER_SECURITY_PROTOCOL: SASL_SSL

      # Producer config: https://docs.confluent.io/current/installation/configuration/producer-configs.html
      PRODUCER_REQUEST_TIMEOUT_MS: 30000 # 30 sec (will retry after that)
      PRODUCER_RETRY_BACKOFF_MS: 2000
      PRODUCER_MAX_BLOCK_MS: 60000
      PRODUCER_LINGER_MS: 1000 # Batch all messages for 1 second before sending out.
      PRODUCER_BUFFER_MEMORY: 33554432 # Max memory used for buffering before blocking for max.block.ms
      PRODUCER_BATCH_SIZE: 500000 # In bytes
      PRODUCER_DELIVERY_TIMEOUT_MS: 120000 # 3 minutes
      PRODUCER_MAX_REQUEST_SIZE: 1048576 # (can be a few batches in 1 request)
      #      PRODUCER_ENABLE_IDEMPOTENCE: "true" # NOT WORKING WITH KALEIDO! Lower the chance of transaction duplicates (they are still possible on Kaleido side)

      PRODUCER_RETRIES: 5
      PRODUCER_MAX_IN_FLIGHT_REQUESTS_PER_CONNECTION: 1 # set to 1 to avoid reordering messages
      PRODUCER_ACKS: "all"
  eth_gateway_events:
    build: ./eth_gateway/events
    entrypoint: |
      bash -c "
      tail -f /dev/null"
#    entrypoint: echo ETH_GATEWAY_EVENTS_ENDPOINT_AUTH_KEY = $ETH_GATEWAY_EVENTS_ENDPOINT_AUTH_KEY
    volumes:
      - ./eth_gateway/events:/app
    ports:
      - 6068:6066
    environment:
      WORKER_PORT: 6066
      KAFKA_BOOTSTRAP_SERVER: ${BROKER_URL}
      BROKER_KEY: ${BROKER_KEY}
      BROKER_SECRET: ${BROKER_SECRET}
      SSL_ENABLED: 1

      SCHEMA_REGISTRY_ENDPOINT: ${SCHEMA_REGISTRY_ENDPOINT}
      SCHEMA_REGISTRY_KEY: ${SCHEMA_REGISTRY_KEY}
      SCHEMA_REGISTRY_SECRET: ${SCHEMA_REGISTRY_SECRET}

      ETH_GATEWAY_EVENTS_ENDPOINT_AUTH_KEY: ${ETH_GATEWAY_EVENTS_ENDPOINT_AUTH_KEY}

  eth_gateway_tunnel:
    build: ./eth_gateway/tunnel
#    entrypoint: |
#      bash -c "
#      tail -f /dev/null"
    entrypoint: lt --port 6066 --subdomain ${ETH_GATEWAY_EVENTS_ENDPOINT_TUNNEL_DOMAIN} --local-host eth_gateway_events
    deploy:
      restart_policy:
        condition: on-failure
        delay: 5s
#        max_attempts: 3
#        window: 120s
